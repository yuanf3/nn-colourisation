{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The following cell sets up all the constants needed for the model to run. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nBW_THRESHOLD = 20 # threshold for detecting colour; used to trim black edges & reject images\n\nIMG_ROWS = 256 # height of input image\nIMG_COLS = 256 # width of input image\nNUM_CLASSES = 512 # number of bins to divide up the RGB colour space\n\nBATCH_SIZE = 24 # size of mini-batch for training\nEPOCHS = 2 # number of epochs to train\nSAVE_INTERVAL = 500 # number of mini-batches before checkpointing model weights\n\nNUM_NEIGHB = 5 # number of nearest neighbours for soft-encoding the probability of a bin\nSIGMA_NEIGHB = 5 # standard deviation for soft-encoding\nTO_REBAL = True # Boolean representing whether to use class rebalancing\n#FACTORS = np.load('factors.npy') # factors for class rebalancing\n\nEPSILON = 1e-4 # small value to prevent log(0)\nTEMPERATURE = 0.8 # parameter for reconstructing colour from a probility distribution\n\n# folders for images\nLINE_PATH = 'lines' # greyscale outlines from processing, used to produce input for the network\nCOLOUR_PATH = 'colours' # colour image from processing, used to produce expected output for the network\n\n# arrays for dividing up the RGB colour space into bins\nCLASS_MAP_B = np.asarray(([32*i+16 for i in range(8)]*64))\nCLASS_MAP_G = np.asarray(([32*int(i/8)+16 for i in range(64)]*8))\nCLASS_MAP_R = np.asarray(([32*int(i/64)+16 for i in range(512)]))\nCLASS_MAP = np.vstack((CLASS_MAP_B, CLASS_MAP_G, CLASS_MAP_R)).T\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell processes images from the dataset and writes output into 'LINE_PATH' and 'COLOUR_PATH'. This results in original colour illustrations being converted into 256x256 images suitable for the network.\nLine drawings and corresponding colour images are written to 'LINE_PATH' and 'COLOUR_PATH' respectively.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\n\nif not os.path.exists(LINE_PATH):\n    os.makedirs(LINE_PATH)\nif not os.path.exists(COLOUR_PATH):\n    os.makedirs(COLOUR_PATH)\n\ndef trim(img):\n    '''Takes an image and trims away black borders.'''\n    # nothing\n    if img.shape[0] == 0:\n        return np.zeros((0, 0, 3))\n    # trim above\n    if np.mean(img[0]) < BW_THRESHOLD:\n        return trim(img[1:])\n    # trim below\n    elif np.mean(img[-1]) < BW_THRESHOLD:\n        return trim(img[:-1])\n    # trim left\n    elif np.mean(img[:, 0]) < BW_THRESHOLD:\n        return trim(img[:, 1:])\n    # trim right\n    elif np.mean(img[:, -1]) < BW_THRESHOLD:\n        return trim(img[:, :-1])\n    return img\n\ndef process(img):\n    '''Takes an image and produces the correctly formatted line and colour versions.'''\n    # shrink dimensions\n    ratio = size / min(img.shape[0], img.shape[1])\n    colour = cv2.resize(img, (0, 0), fx=ratio, fy=ratio, interpolation=cv2.INTER_AREA)\n    \n    # convert to greyscale\n    grey = cv2.cvtColor(colour, cv2.COLOR_BGR2GRAY)\n    \n    # erode, dilate & subtract\n    kernel = np.ones((2, 2), np.uint8)\n    erosion = cv2.erode(grey, kernel, iterations=1)\n    dilation = cv2.dilate(grey, kernel, iterations=1)\n    diff = cv2.absdiff(erosion, dilation)\n    diff = cv2.multiply(diff, 2)\n    line = cv2.bitwise_not(diff)\n    \n    return line, colour\n\ndir_name = '/kaggle/input/tagged-anime-illustrations/danbooru-images/danbooru-images'\nfile_paths = []\nfor (dir_path, _, file_names) in os.walk(dir_name):\n    file_paths += [os.path.join(dir_path, file) for file in file_names]\n\n# process every jpg image\nsize = min(IMG_ROWS, IMG_COLS)\nfor index, file_path in enumerate(file_paths):\n    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n    \n    # trim black borders\n    img = trim(img)\n    \n    # check if image is too small\n    h, w, c = img.shape\n    h = float(h)\n    w = float(w)\n    if h < size or w < size:\n        # print(file_path, ': small')\n        continue\n    \n    # create cropped square versions (tiles)\n    num_slides, hskip, wskip, min_dim = 1, 0, 0, h\n    if h > w:\n        num_slides = np.ceil(h/w)\n        hskip = w - (w*num_slides-h) / (num_slides-1)\n        min_dim = w\n    elif h < w:\n        num_slides = np.ceil(w/h)\n        wskip = h - (h*num_slides-w) / (num_slides-1)\n        min_dim = h\n        \n    imgs = []\n    for n in range(int(num_slides)):\n        imgs.append(img[int(hskip*n):int(hskip*n+min_dim), \\\n                        int(wskip*n):int(wskip*n+min_dim)])\n        \n    for crop_num, img in enumerate(imgs):\n        img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n        # check if image is too dull using mean saturation\n        mean_sat = cv2.mean(img_hsv)\n        if mean_sat[1] < BW_THRESHOLD:\n            # print(file_path, ': dull')\n            continue\n        \n        # image is suitable for processing\n        line, colour = process(img)\n        \n        # write processed images to file\n        out_name = str(index) + '-' + str(crop_num) + '.jpg'\n        cv2.imwrite(os.path.join(LINE_PATH, out_name), line)\n        cv2.imwrite(os.path.join(COLOUR_PATH, out_name), colour)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell performs the final tasks of handling the image data. Specifically, the cell:\n* Defines a function 'soft_encode', which takes in a colour image and produces a probability distribution for each pixel. Each pixel's probability distribution represents the chance that the pixel's colour belongs to a particular class defined by the CLASS_MAP. This represents the expected output from the network. Optionally, class rebalancing is applied to the probability distribution.\n* Specifies a custom DataGenerator for producing mini-batches of data; needed to reduce RAM requirements.\n* Splits up the data set into training and validation sets by assigning the file names into 'train_names.txt' or 'valid_names.txt' respectively.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport random\nimport sklearn.neighbors as nn\nfrom keras.utils import Sequence\n\ndef soft_encode(bgr):\n    ''' Takes in a colour image and produces a 'soft-encoded' probability \n    distribution of colour classes for each pixel. Also rebalances each pixel \n    based on how common its colour class is.'''\n    # arrange image array (assumes BGR because of cv2 peculiarities)\n    h, w = bgr.shape[:2]\n    pixels = h * w\n    bgr = bgr.reshape((pixels, 3))\n    \n    # for each pixel, find the 5 nearest neighbours with respect to CLASS_MAP\n    neighb_finder = nn.NearestNeighbors(NUM_NEIGHB, algorithm='ball_tree').fit(CLASS_MAP)\n    dist_neighb, index_neighb = neighb_finder.kneighbors(bgr)\n    \n    # smooth the distances using a gaussian kernel\n    weights = np.exp(-dist_neighb ** 2 / (2 * SIGMA_NEIGHB ** 2))\n    weights = weights / np.sum(weights, axis=1, keepdims=True)\n    \n    # multiply the weights by a factor for class rebalancing\n    if TO_REBAL:\n        for i in range(pixels):\n            one_hot = index_neighb[i][0]\n            weights[i] *= FACTORS[one_hot]\n        \n    # format the probability distribution\n    encode = np.zeros((pixels, NUM_CLASSES))\n    index_pts = np.arange(pixels).reshape(pixels, 1)\n    encode[index_pts, index_neighb] = weights\n    encode = encode.reshape(h, w, NUM_CLASSES)\n    \n    return encode\n\nclass DataGenerator(Sequence):\n    ''' Produces mini-batches for training & evaluation, since the dataset \n    takes too much memory to load at once'''\n    def __init__(self, names_file):\n        with open(names_file, 'r') as f:\n            self.names = f.read().splitlines()\n        np.random.shuffle(self.names)\n\n    def __len__(self):\n        return int(np.ceil(len(self.names) / float(BATCH_SIZE)))\n\n    def __getitem__(self, index):\n        start = index * BATCH_SIZE\n\n        out_rows, out_cols = IMG_ROWS // 4, IMG_COLS // 4\n\n        length = min(BATCH_SIZE, (len(self.names) - start))\n        X = np.empty((length, IMG_ROWS, IMG_COLS, 1), dtype=np.float32)\n        y = np.empty((length, out_rows, out_cols, NUM_CLASSES), dtype=np.float32)\n\n        for i in range(length):\n            name = self.names[start]\n            line_filename = os.path.join(LINE_PATH, name)\n            colour_filename = os.path.join(COLOUR_PATH, name)\n            \n            line = cv2.imread(line_filename, cv2.IMREAD_GRAYSCALE)\n            line = (line-0) / 255 # substract mean\n            \n            bgr = cv2.imread(colour_filename, cv2.IMREAD_COLOR)\n            bgr = cv2.resize(bgr, (out_rows, out_cols), cv2.INTER_AREA)\n            encode = soft_encode(bgr)\n            \n            # 50% chance to randomly flip the image\n            if np.random.random_sample() > 0.5:\n                line = np.fliplr(line)\n                encode = np.fliplr(encode)\n\n            X[i, :, :, 0] = line\n            y[i] = encode\n            \n            start += 1\n\n        return X, y\n\n    def on_epoch_end(self):\n        np.random.shuffle(self.names)\n    \ndef split_data():\n    ''' Splits the data set into training & validation sets.'''\n    names = [f for f in os.listdir(LINE_PATH) if f.lower().endswith('.jpg')]\n\n    num_samples = len(names) # 351204\n    num_train_samples = int(num_samples * 0.992) # 348394\n    num_valid_samples = num_samples - num_train_samples # 2810\n    print('num samples: ' + str(num_samples))\n    print('num train samples: ' + str(num_train_samples))\n    print('num valid samples: ' + str(num_valid_samples))\n    \n    valid_names = random.sample(names, num_valid_samples)\n    train_names = [n for n in names if n not in valid_names]\n    np.random.shuffle(train_names)\n    np.random.shuffle(valid_names)\n\n    with open('train_names.txt', 'w') as file:\n        file.write('\\n'.join(train_names))\n\n    with open('valid_names.txt', 'w') as file:\n        file.write('\\n'.join(valid_names))\n        \nsplit_data()\n","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell defines the network architecture. The network takes a 256x256x1 input array which represents a 256x256 grayscale line drawing. After several convolutional blocks, the network outputs a 64x64x512 array, which represents a 64x64 square of pixels, each containing a probability distribution over the 512 classes that the colour space has been divided into. This is later upsampled to produce a colour prediction.","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input, Conv2D, BatchNormalization, UpSampling2D\nfrom keras.models import Model\n\ndef build_model():\n    inputs = Input(shape=(IMG_ROWS, IMG_COLS, 1))\n    \n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_1',\n               kernel_initializer=\"he_normal\")(inputs)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_2',\n               kernel_initializer=\"he_normal\", strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_1',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_2',\n               kernel_initializer=\"he_normal\", strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_1',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_2',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_3',\n               kernel_initializer=\"he_normal\", strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_1',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_2',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_3',\n               kernel_initializer=\"he_normal\")(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', dilation_rate=2, name='conv5_1',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', dilation_rate=2, name='conv5_2',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', dilation_rate=2, name='conv5_3',\n               kernel_initializer=\"he_normal\")(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', dilation_rate=2, name='conv6_1',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', dilation_rate=2, name='conv6_2',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', dilation_rate=2, name='conv6_3',\n               kernel_initializer=\"he_normal\")(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv7_1',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv7_2',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv7_3',\n               kernel_initializer=\"he_normal\")(x)\n    x = BatchNormalization()(x)\n\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv8_1',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv8_2',\n               kernel_initializer=\"he_normal\")(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv8_3',\n               kernel_initializer=\"he_normal\")(x)\n\n    outputs = Conv2D(NUM_CLASSES, (1, 1), activation='softmax', padding='same', name='pred')(x)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"ClassificationModel\")\n    return model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell builds, compiles and fits (i.e. trains) a network. The custom callbacks are used to save model weights after every nth batch (as specified by SAVE_INTERVAL), and after every training epoch.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport keras\n\nclass loss_tracker(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.cumul_batches = 0\n\n    def on_batch_end(self, batch, logs={}):\n        loss = logs.get('loss')\n        self.losses.append(loss)\n        # save weights every nth batch\n        self.cumul_batches += 1\n        if self.cumul_batches % SAVE_INTERVAL == 0:\n            self.model.save_weights('model_batch{}.hd5'.format(self.cumul_batches))\n        \nclass save_epoch(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        self.model.save_weights('model_epoch{}.hd5'.format(epoch))\n\n# build and compile a new model\nnew_model = build_model()\nnew_model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy')\n\n# set up callbacks\nloss_tracker = loss_tracker()\nsave_epoch = save_epoch()\ncallbacks = [loss_tracker, save_epoch]\n\nprint(new_model.summary())\n\n# set up generators for training and validation data\ntrain_gen = DataGenerator('train_names.txt')\nvalid_gen = DataGenerator('valid_names.txt')\n\n# train a network model\nnew_model.fit_generator(generator=train_gen,\n                        validation_data=valid_gen,\n                        epochs=EPOCHS,\n                        verbose=1,\n                        callbacks=callbacks\n                        )\n\nkeras.backend.clear_session()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell evaluates each saved checkpoint, and determines the one with the lowest validation loss.","metadata":{}},{"cell_type":"code","source":"# evaluate each set of saved weights to determine validation loss\nlosses = loss_tracker.losses.copy()\nlength = len(losses)\nval_losses = []\nresults = []\nsaved_weights = [('model_batch'+str(i)+'.hd5') for i in range(SAVE_INTERVAL, length, SAVE_INTERVAL)]\nfor weights in saved_weights:\n    test = build_model()\n    test.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy')\n    test.load_weights(weights)\n    val_loss = test.evaluate_generator(valid_gen, verbose=1)\n    val_losses.append(val_loss)\n    results.append(weights, val_loss)\n    print(weights, ':', val_loss)\n    keras.backend.clear_session()\nsorted_results = sorted(results, key=lambda x: x[1])\nval_loss_best = sorted_results[0][1]\nbest_model = sorted_results[0][0]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell plots a graph showing the training loss throughout training and the validation loss of each saved checkpoint.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# generate loss graph\nx = np.arange(length)\nx_interval = np.arange(SAVE_INTERVAL, length, SAVE_INTERVAL)\nx_best = val_losses.index(val_loss_best) * SAVE_INTERVAL + SAVE_INTERVAL\n\nfig, ax = plt.subplots(figsize=(12, 9))\n\np1 = ax.plot(x, losses, linewidth=0.2, zorder=1)\np2 = ax.scatter(x_interval, val_losses, color='r', marker='o', zorder=2)\np3 = ax.scatter(x_best, val_loss_best, color='lime',  marker='o', zorder=3)\n\ny_min = np.floor(min(losses))\ny_max = np.ceil(max(losses))\nax.set_ylim([y_min, y_max])\nax.set_xlabel('batch number')\nax.set_ylabel('loss')\nax.set_title('Model loss')\nax.legend([p1[0], p2, p3], ['training loss', 'validation loss', 'lowest validation loss'])\n\nplt.savefig('graph.png')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell:\n* Loads a trained model\n* Reads in line drawings from 'test_X'\n* Uses the model to make colour predictions and writes them into 'test_y'","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom keras.models import load_model\n\ndef get_prediction(model, test):\n    holder = np.empty((1, IMG_ROWS, IMG_COLS, 1))\n    holder[0, :, :, 0] = test / 255\n    \n    z = model.predict(holder)[0]\n    z = np.reshape(z, (64**2, NUM_CLASSES))\n\n    probs = np.exp(np.log(z + EPSILON) / TEMPERATURE)\n    probs = probs / np.sum(probs, axis=1, keepdims=True)\n\n    out_img = np.stack((np.sum(CLASS_MAP_B * probs, axis=1), \n                        np.sum(CLASS_MAP_G * probs, axis=1), \n                        np.sum(CLASS_MAP_R * probs, axis=1)), axis=1)\n\n    out_img = np.reshape(out_img, (64, 64, 3))\n    out_img = cv2.resize(out_img, (IMG_ROWS, IMG_COLS), interpolation=cv2.INTER_LINEAR)\n    \n    return out_img\n\nnew_model = load_model(best_model) # THIS CAN BE CHANGED AS NECESSARY\nX = 'test_X'\ny = 'test_y'\n\nif not os.path.exists(y):\n    os.makedirs(y)\n\nfor filename in os.listdir(X):\n    test = cv2.imread(os.path.join(X, filename), cv2.IMREAD_GRAYSCALE)\n    lines = cv2.imread(os.path.join(X, filename), cv2.IMREAD_COLOR)\n    \n    predicted_colours = get_prediction(new_model, test)\n    \n    # overlay lines on top of predicted colours\n    result = predicted_colours * (lines / 255)\n    \n    cv2.imwrite(os.path.join(y, filename), result)\n","metadata":{},"execution_count":null,"outputs":[]}]}
